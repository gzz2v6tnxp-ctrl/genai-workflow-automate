"""
COV-RAG Graph: Workflow LangGraph pour RAG avec Chain-of-Verification

Ce module int√®gre le syst√®me COV-RAG dans un workflow LangGraph
pour une orchestration robuste avec v√©rification des hallucinations.

Pipeline:
1. retrieve_with_rerank: R√©cup√©ration hybride + re-ranking
2. generate_initial: G√©n√©ration avec ancrage strict
3. extract_claims: Extraction des affirmations v√©rifiables
4. verify_claims: V√©rification CoVE contre les sources
5. correct_if_needed: Correction des hallucinations d√©tect√©es
6. evaluate_final: √âvaluation qualit√© finale

Auteur: GenAI Workflow Automation
"""

import sys
import re
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import uuid

# Ajouter le r√©pertoire racine au path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from langgraph.graph import END, StateGraph
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

from agents.state import COVRAGGraphState
from agents.cov_rag import COVRAGRetriever, ChainOfVerification
from scripts import config


# ============================================================================
# D√âTECTION DE LANGUE
# ============================================================================

FRENCH_MARKERS = {
    "carte", "bancaire", "compte", "probl√®me", "bonjour", "merci", 
    "solde", "cr√©dit", "bloqu√©e", "facturation", "opposition",
    "comment", "pourquoi", "quand", "combien", "quel", "quelle"
}


def detect_language(text: str) -> str:
    """D√©tecte la langue (fr/en) de mani√®re robuste."""
    lower = text.lower()
    
    # V√©rifier les accents fran√ßais
    if re.search(r"[√†√¢√ß√©√®√™√´√Æ√Ø√¥√π√ª√º√ø≈ì]", lower):
        return "fr"
    
    # V√©rifier les mots fran√ßais
    hits = sum(1 for w in FRENCH_MARKERS if w in lower)
    if hits >= 2:
        return "fr"
    
    return "en"


# ============================================================================
# N≈íUDS DU GRAPHE COV-RAG
# ============================================================================

# Initialisation globale (lazy loading)
_retriever: Optional[COVRAGRetriever] = None
_llm: Optional[ChatOpenAI] = None
_cove: Optional[ChainOfVerification] = None


def _get_retriever(collection_name: str = "demo_public") -> COVRAGRetriever:
    """R√©cup√®re ou initialise le retriever."""
    global _retriever
    if _retriever is None or _retriever.collection_name != collection_name:
        _retriever = COVRAGRetriever(
            collection_name=collection_name,
            use_cloud=True,
            top_k=5,
            score_threshold=0.35
        )
    return _retriever


def _get_llm() -> ChatOpenAI:
    """R√©cup√®re ou initialise le LLM."""
    global _llm
    if _llm is None:
        _llm = ChatOpenAI(
            model=config.OPENAI_MODEL,
            temperature=float(getattr(config, "OPENAI_TEMPERATURE", 0.2)),
            max_tokens=int(getattr(config, "OPENAI_MAX_TOKENS", 1024)),
            api_key=config.OPENAI_API_KEY
        )
    return _llm


def _get_cove(collection_name: str = "demo_public") -> ChainOfVerification:
    """R√©cup√®re ou initialise le module CoVE."""
    global _cove
    if _cove is None:
        _cove = ChainOfVerification(
            llm=_get_llm(),
            retriever=_get_retriever(collection_name),
            verification_threshold=0.7,
            max_claims_to_verify=5
        )
    return _cove


# ----------------------------------------------------------------------------
# N≈ìud 1: R√©cup√©ration avec Re-ranking
# ----------------------------------------------------------------------------

def retrieve_with_rerank(state: COVRAGGraphState) -> Dict[str, Any]:
    """
    R√©cup√®re les documents avec recherche hybride et re-ranking.
    
    √âtapes:
    1. R√©cup√©ration hybride (dense + MMR)
    2. Re-ranking par pertinence
    3. Formatage pour le contexte
    """
    print("---[1] R√âCUP√âRATION + RE-RANKING---")
    
    question = state["question"]
    collection = state.get("collection", "demo_public")
    sources_filter = state.get("sources_filter")
    
    print(f"Question: {question}")
    print(f"Collection: {collection}")
    
    try:
        retriever = _get_retriever(collection)
        
        # Construire les filtres
        filters = None
        if collection == "knowledge_base_main" and sources_filter:
            allowed = {"synth", "cfpb", "enron"}
            filtered_values = [s for s in sources_filter if s in allowed]
            if filtered_values:
                filters = {"source": filtered_values}
        
        # R√©cup√©ration hybride
        docs = retriever.hybrid_retrieve(question, filters)
        
        if not docs:
            print("‚ö†Ô∏è Aucun document trouv√©")
            return {
                "documents": [],
                "sources": [],
                "reranked_documents": [],
                "reranked_sources": [],
                "grade": "not_relevant"
            }
        
        # Re-ranking
        reranked_docs = retriever.rerank(question, docs)
        
        # Formatage
        documents = []
        sources = []
        reranked_documents = []
        reranked_sources = []
        
        for i, doc in enumerate(reranked_docs, 1):
            score = doc.metadata.get("score", 0.0)
            doc_id = doc.metadata.get("id", f"doc_{i}")
            
            doc_text = f"[{doc_id}] (Score: {score:.3f})\n{doc.page_content}"
            documents.append(doc_text)
            reranked_documents.append(doc_text)
            
            source_info = {
                "id": doc_id,
                "score": score,
                "source": doc.metadata.get("source", "unknown"),
                "lang": doc.metadata.get("lang", "unknown"),
                "content_preview": doc.page_content[:200]
            }
            sources.append(source_info)
            reranked_sources.append(source_info)
        
        print(f"‚úÖ {len(documents)} document(s) r√©cup√©r√©s et re-class√©s")
        print(f"Scores: {reranked_docs[0].metadata.get('score', 0):.3f} - {reranked_docs[-1].metadata.get('score', 0):.3f}")
        
        # √âvaluer la pertinence
        best_score = sources[0]["score"] if sources else 0
        grade = "relevant" if best_score >= 0.5 else ("marginal" if best_score >= 0.35 else "not_relevant")
        
        return {
            "documents": documents,
            "sources": sources,
            "reranked_documents": reranked_documents,
            "reranked_sources": reranked_sources,
            "grade": grade,
            "similarity_score": best_score
        }
        
    except Exception as e:
        print(f"‚ùå Erreur r√©cup√©ration: {e}")
        return {
            "documents": [],
            "sources": [],
            "reranked_documents": [],
            "reranked_sources": [],
            "grade": "not_relevant",
            "error": str(e)
        }


# ----------------------------------------------------------------------------
# N≈ìud 2: G√©n√©ration Initiale avec Ancrage
# ----------------------------------------------------------------------------

def generate_initial(state: COVRAGGraphState) -> Dict[str, Any]:
    """
    G√©n√®re la r√©ponse initiale avec ancrage strict sur les sources.
    """
    print("---[2] G√âN√âRATION INITIALE---")
    
    question = state["question"]
    documents = state.get("reranked_documents", state.get("documents", []))
    sources = state.get("reranked_sources", state.get("sources", []))
    
    if not documents:
        return {"generation": "", "initial_generation": ""}
    
    # D√©tecter la langue
    lang = detect_language(question)
    print(f"Langue d√©tect√©e: {lang}")
    
    # Construire le contexte avec IDs
    context_parts = []
    for doc, src in zip(documents, sources):
        doc_id = src.get("id", "unknown")
        score = src.get("score", 0.0)
        # Extraire le contenu sans le header
        content = doc.split("\n", 1)[-1] if "\n" in doc else doc
        context_parts.append(f"[{doc_id}] (score: {score:.3f})\n{content[:600]}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    # Prompt avec ancrage strict
    if lang == "fr":
        system_prompt = """Tu es un assistant bancaire professionnel.

R√àGLES D'ANCRAGE STRICTES:
1. Utilise UNIQUEMENT les informations des documents fournis
2. Cite TOUJOURS la source [ID] quand tu utilises une information  
3. Si l'information n'est pas dans les sources, dis-le clairement
4. NE PAS inventer de dates, montants, num√©ros ou donn√©es
5. Reste factuel et pr√©cis

R√©ponds de mani√®re professionnelle et empathique."""
        
        user_template = """Documents sources:
{context}

Question: {question}

R√©ponds en fran√ßais en citant les sources [ID] utilis√©es."""
    else:
        system_prompt = """You are a professional banking assistant.

STRICT GROUNDING RULES:
1. Use ONLY information from the provided documents
2. ALWAYS cite the source [ID] when using information
3. If information is not in sources, say so clearly
4. DO NOT invent dates, amounts, numbers or data
5. Stay factual and precise

Respond professionally and empathetically."""
        
        user_template = """Source documents:
{context}

Question: {question}

Reply in English, citing sources [ID] used."""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", user_template)
    ])
    
    llm = _get_llm()
    chain = prompt | llm
    
    try:
        response = chain.invoke({
            "context": context,
            "question": question
        })
        generation = response.content
        
        print(f"‚úÖ R√©ponse g√©n√©r√©e ({len(generation)} caract√®res)")
        
        return {
            "generation": generation,
            "initial_generation": generation,
            "response_lang": lang
        }
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration: {e}")
        fallback = (
            "D√©sol√©, une erreur s'est produite." if lang == "fr"
            else "Sorry, an error occurred."
        )
        return {
            "generation": fallback,
            "initial_generation": fallback,
            "response_lang": lang,
            "error": str(e)
        }


# ----------------------------------------------------------------------------
# N≈ìud 3: Extraction des Affirmations (CoVE Step 1)
# ----------------------------------------------------------------------------

def extract_claims(state: COVRAGGraphState) -> Dict[str, Any]:
    """
    Extrait les affirmations v√©rifiables de la r√©ponse g√©n√©r√©e.
    """
    print("---[3] EXTRACTION DES AFFIRMATIONS---")
    
    generation = state.get("generation", "")
    cove_enabled = state.get("cove_enabled", True)
    
    if not cove_enabled or not generation:
        return {"claims_extracted": [], "cove_enabled": cove_enabled}
    
    llm = _get_llm()
    
    extract_prompt = ChatPromptTemplate.from_messages([
        ("system", """Extrais les affirmations factuelles v√©rifiables de cette r√©ponse.

Pour chaque affirmation:
- Le fait pr√©cis √©nonc√©
- La cat√©gorie: "numerical", "temporal", "entity", "factual"

Retourne UNIQUEMENT un tableau JSON:
[{"fact": "...", "category": "..."}, ...]

Ignore les conseils g√©n√©raux et formulations vagues."""),
        ("human", "R√©ponse:\n{response}")
    ])
    
    chain = extract_prompt | llm
    
    try:
        result = chain.invoke({"response": generation})
        content = result.content.strip()
        
        # Nettoyer le JSON
        content = re.sub(r'```json\s*', '', content)
        content = re.sub(r'```\s*', '', content)
        
        # Trouver le tableau JSON
        start = content.find('[')
        end = content.rfind(']')
        if start != -1 and end != -1:
            content = content[start:end + 1]
        
        claims = json.loads(content)
        
        print(f"‚úÖ {len(claims)} affirmation(s) extraite(s)")
        
        return {"claims_extracted": claims[:5]}  # Limiter √† 5
        
    except (json.JSONDecodeError, Exception) as e:
        print(f"‚ö†Ô∏è Erreur extraction (fallback): {e}")
        
        # Fallback: extraction bas√©e sur des r√®gles
        claims = []
        sentences = re.split(r'[.!?]\s+', generation)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 20:
                continue
            
            # Ignorer les phrases g√©n√©riques
            if re.search(r'^(je|vous|nous|voici|n\'h√©sitez)', sentence, re.I):
                continue
            
            category = "factual"
            if re.search(r'\b\d+[,.]?\d*\s*[‚Ç¨$%]', sentence):
                category = "numerical"
            elif re.search(r'\b(19|20)\d{2}\b', sentence):
                category = "temporal"
            
            claims.append({"fact": sentence, "category": category})
        
        return {"claims_extracted": claims[:5]}


# ----------------------------------------------------------------------------
# N≈ìud 4: V√©rification des Affirmations (CoVE Step 2-3)
# ----------------------------------------------------------------------------

def verify_claims(state: COVRAGGraphState) -> Dict[str, Any]:
    """
    V√©rifie chaque affirmation contre les sources.
    """
    print("---[4] V√âRIFICATION CoVE---")
    
    claims = state.get("claims_extracted", [])
    documents = state.get("reranked_documents", state.get("documents", []))
    sources = state.get("reranked_sources", state.get("sources", []))
    cove_enabled = state.get("cove_enabled", True)
    
    if not cove_enabled or not claims:
        return {
            "verification_results": [],
            "hallucination_detected": False,
            "cove_confidence": 1.0
        }
    
    llm = _get_llm()
    
    # Pr√©parer le texte des sources
    sources_text = "\n\n---\n\n".join([
        f"[{src.get('id', i)}]\n{doc}"
        for i, (doc, src) in enumerate(zip(documents, sources))
    ])
    
    verify_prompt = ChatPromptTemplate.from_messages([
        ("system", """V√©rifie si l'affirmation est correcte selon les sources.

R√àGLES:
1. "is_verified": true SEULEMENT si explicitement support√© par les sources
2. Si l'info n'est pas dans les sources: is_verified = false
3. Pour chiffres/dates: doivent correspondre exactement
4. Cite l'evidence exacte

Retourne UNIQUEMENT un objet JSON:
{{"is_verified": true/false, "confidence": 0.0-1.0, "evidence": "...", "correction": "..." ou null}}"""),
        ("human", """Affirmation: {claim}

Sources:
{sources}""")
    ])
    
    chain = verify_prompt | llm
    
    verification_results = []
    hallucination_detected = False
    
    for claim_data in claims:
        claim = claim_data.get("fact", str(claim_data))
        
        try:
            result = chain.invoke({
                "claim": claim,
                "sources": sources_text
            })
            
            content = result.content.strip()
            # Nettoyer
            content = re.sub(r'```json\s*', '', content)
            content = re.sub(r'```\s*', '', content)
            
            start = content.find('{')
            end = content.rfind('}')
            if start != -1 and end != -1:
                content = content[start:end + 1]
            
            verification = json.loads(content)
            
            verification_results.append({
                "claim": claim,
                "is_verified": verification.get("is_verified", False),
                "confidence": verification.get("confidence", 0.5),
                "evidence": verification.get("evidence", ""),
                "correction": verification.get("correction")
            })
            
            if not verification.get("is_verified", False):
                hallucination_detected = True
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur v√©rification claim: {e}")
            verification_results.append({
                "claim": claim,
                "is_verified": False,
                "confidence": 0.3,
                "evidence": "V√©rification impossible",
                "correction": None
            })
            hallucination_detected = True
    
    # Calculer la confiance CoVE
    if verification_results:
        verified_count = sum(1 for v in verification_results if v["is_verified"])
        cove_confidence = verified_count / len(verification_results)
    else:
        cove_confidence = 1.0
    
    print(f"‚úÖ V√©rification: {sum(1 for v in verification_results if v['is_verified'])}/{len(verification_results)}")
    print(f"Hallucination d√©tect√©e: {hallucination_detected}")
    
    return {
        "verification_results": verification_results,
        "hallucination_detected": hallucination_detected,
        "cove_confidence": cove_confidence
    }


# ----------------------------------------------------------------------------
# N≈ìud 5: Correction si N√©cessaire (CoVE Step 4)
# ----------------------------------------------------------------------------

def correct_if_needed(state: COVRAGGraphState) -> Dict[str, Any]:
    """
    Corrige la r√©ponse si des hallucinations ont √©t√© d√©tect√©es.
    """
    print("---[5] CORRECTION (si n√©cessaire)---")
    
    hallucination_detected = state.get("hallucination_detected", False)
    verification_results = state.get("verification_results", [])
    initial_generation = state.get("initial_generation", "")
    generation = state.get("generation", "")
    documents = state.get("reranked_documents", state.get("documents", []))
    question = state["question"]
    lang = state.get("response_lang", "fr")
    
    # Compter les corrections n√©cessaires
    needs_correction = any(not v["is_verified"] for v in verification_results)
    
    if not needs_correction:
        print("‚úÖ Aucune correction n√©cessaire")
        return {
            "generation": generation,
            "corrections_made": 0
        }
    
    print("‚ö†Ô∏è Correction en cours...")
    
    llm = _get_llm()
    
    # Formater les r√©sultats de v√©rification
    results_text = "\n".join([
        f"- \"{v['claim'][:100]}...\"\n"
        f"  V√©rifi√©: {'‚úÖ' if v['is_verified'] else '‚ùå'} (confiance: {v['confidence']:.0%})\n"
        f"  Correction: {v['correction'] or 'N/A'}"
        for v in verification_results
    ])
    
    sources_text = "\n\n".join(documents[:3])
    
    if lang == "fr":
        system_prompt = """Corrige la r√©ponse en tenant compte des v√©rifications.

R√àGLES:
1. Conserve les parties v√©rifi√©es correctes
2. Corrige ou supprime les affirmations incorrectes
3. Ne rajoute PAS de nouvelles informations
4. Maintiens un ton professionnel
5. Cite les sources quand appropri√©"""
    else:
        system_prompt = """Correct the response based on verifications.

RULES:
1. Keep verified correct parts
2. Fix or remove incorrect claims
3. Do NOT add new information
4. Maintain professional tone
5. Cite sources when appropriate"""
    
    correct_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", """Question: {question}

R√©ponse initiale:
{initial_response}

R√©sultats de v√©rification:
{verification_results}

Sources:
{sources}

G√©n√®re la r√©ponse corrig√©e:""")
    ])
    
    chain = correct_prompt | llm
    
    try:
        result = chain.invoke({
            "question": question,
            "initial_response": initial_generation,
            "verification_results": results_text,
            "sources": sources_text
        })
        
        corrected = result.content
        corrections_made = sum(1 for v in verification_results if not v["is_verified"])
        
        print(f"‚úÖ {corrections_made} correction(s) appliqu√©e(s)")
        
        return {
            "generation": corrected,
            "corrections_made": corrections_made
        }
        
    except Exception as e:
        print(f"‚ùå Erreur correction: {e}")
        return {
            "generation": generation,
            "corrections_made": 0,
            "error": str(e)
        }


# ----------------------------------------------------------------------------
# N≈ìud 6: √âvaluation Finale
# ----------------------------------------------------------------------------

def evaluate_final(state: COVRAGGraphState) -> Dict[str, Any]:
    """
    √âvalue la qualit√© finale de la r√©ponse.
    """
    print("---[6] √âVALUATION FINALE---")
    
    generation = state.get("generation", "")
    documents = state.get("documents", [])
    sources = state.get("sources", [])
    verification_results = state.get("verification_results", [])
    cove_confidence = state.get("cove_confidence", 1.0)
    similarity_score = state.get("similarity_score", 0.0)
    question = state.get("question", "")
    
    # Score de similarit√© des sources
    scores = [float(s.get("score", 0.0)) for s in sources] if sources else [0.0]
    avg_similarity = sum(scores) / len(scores) if scores else 0.0
    
    # V√©rification des citations
    doc_text = " ".join(documents).lower()
    cited_ids = []
    for s in sources:
        sid = str(s.get("id", ""))
        if sid and sid in generation:
            cited_ids.append(sid)
    cites_ok = len(cited_ids) > 0
    
    # D√©tection d'hallucination basique (dates/montants)
    years = re.findall(r"\b(19|20)\d{2}\b", generation)
    amounts = re.findall(r"\b\d+[,.]?\d*\s*[‚Ç¨$]\b", generation, re.I)
    
    basic_hallucination = False
    for y in years:
        if y not in doc_text:
            basic_hallucination = True
            break
    
    # Score de confiance final
    # Combinaison: 40% similarit√©, 40% CoVE, 20% citations
    confidence_score = (
        0.4 * avg_similarity +
        0.4 * cove_confidence +
        0.2 * (1.0 if cites_ok else 0.5)
    )
    
    # D√©cision qualit√©
    hallucination_detected = state.get("hallucination_detected", False) or basic_hallucination
    quality_pass = (
        not hallucination_detected and
        confidence_score >= 0.4 and
        (cites_ok or cove_confidence >= 0.7)
    )
    
    escalate = confidence_score < 0.3 or (hallucination_detected and cove_confidence < 0.5)
    
    # Log des m√©triques
    metrics = {
        "timestamp": datetime.utcnow().isoformat(),
        "question": question,
        "similarity_score": round(avg_similarity, 3),
        "cove_confidence": round(cove_confidence, 3),
        "final_confidence": round(confidence_score, 3),
        "cites_ok": cites_ok,
        "hallucination_detected": hallucination_detected,
        "quality_pass": quality_pass,
        "escalate": escalate,
        "corrections_made": state.get("corrections_made", 0),
        "num_sources": len(sources),
        "num_verifications": len(verification_results)
    }
    
    # Sauvegarder les m√©triques
    logs_dir = project_root / "logs"
    logs_dir.mkdir(parents=True, exist_ok=True)
    
    with open(logs_dir / "cov_rag_metrics.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(metrics, ensure_ascii=False) + "\n")
    
    # Sauvegarder snapshot si √©chec
    if not quality_pass:
        snaps_dir = project_root / "snapshots" / "for_review"
        snaps_dir.mkdir(parents=True, exist_ok=True)
        
        snapshot = {
            "id": uuid.uuid4().hex,
            "timestamp": datetime.utcnow().isoformat(),
            "question": question,
            "generation": generation,
            "initial_generation": state.get("initial_generation", ""),
            "verification_results": verification_results,
            "metrics": metrics
        }
        
        snap_path = snaps_dir / f"{snapshot['id']}.json"
        with open(snap_path, "w", encoding="utf-8") as f:
            json.dump(snapshot, f, ensure_ascii=False, indent=2)
        
        print(f"‚ö†Ô∏è Snapshot saved: {snap_path.name}")
    
    print(f"üìä Confiance finale: {confidence_score:.0%}")
    print(f"   Quality pass: {quality_pass}, Escalate: {escalate}")
    
    return {
        "similarity_score": avg_similarity,
        "confidence_score": confidence_score,
        "final_confidence": confidence_score,
        "quality_pass": quality_pass,
        "escalate": escalate,
        "cites_ok": cites_ok,
        "hallucination_detected": hallucination_detected
    }


# ----------------------------------------------------------------------------
# N≈ìud: Fallback
# ----------------------------------------------------------------------------

def fallback_response(state: COVRAGGraphState) -> Dict[str, Any]:
    """R√©ponse de secours si pas de documents pertinents."""
    print("---FALLBACK---")
    
    question = state.get("question", "")
    lang = detect_language(question)
    
    if lang == "fr":
        generation = (
            "Je n'ai pas trouv√© d'informations suffisamment pertinentes pour r√©pondre "
            "√† votre question de mani√®re fiable. Je vous recommande de contacter "
            "notre service client pour une assistance personnalis√©e."
        )
    else:
        generation = (
            "I couldn't find sufficiently relevant information to answer your question "
            "reliably. I recommend contacting our customer service for personalized assistance."
        )
    
    return {
        "generation": generation,
        "sources": [],
        "response_lang": lang,
        "quality_pass": False,
        "confidence_score": 0.0
    }


# ----------------------------------------------------------------------------
# N≈ìud: Human Review Escalation
# ----------------------------------------------------------------------------

def human_review(state: COVRAGGraphState) -> Dict[str, Any]:
    """Escalade vers revue humaine."""
    print("---HUMAN REVIEW ESCALATION---")
    
    question = state.get("question", "")
    lang = detect_language(question)
    
    if lang == "fr":
        msg = (
            "Nous ne pouvons pas fournir une r√©ponse automatis√©e suffisamment fiable. "
            "Votre demande a √©t√© transf√©r√©e √† nos sp√©cialistes qui vous contacteront sous peu."
        )
    else:
        msg = (
            "We cannot provide a sufficiently reliable automated response. "
            "Your request has been escalated to our specialists who will contact you shortly."
        )
    
    return {
        "generation": msg,
        "escalated": True,
        "response_lang": lang
    }


# ============================================================================
# CONSTRUCTION DU GRAPHE
# ============================================================================

def decide_after_retrieval(state: COVRAGGraphState) -> str:
    """D√©cide le chemin apr√®s la r√©cup√©ration."""
    grade = state.get("grade", "not_relevant")
    if grade == "relevant":
        return "generate"
    elif grade == "marginal":
        return "generate"  # Tenter quand m√™me
    else:
        return "fallback"


def decide_after_evaluation(state: COVRAGGraphState) -> str:
    """D√©cide le chemin apr√®s l'√©valuation finale."""
    quality_pass = state.get("quality_pass", False)
    escalate = state.get("escalate", False)
    
    if quality_pass:
        return "end"
    elif escalate:
        return "human_review"
    else:
        return "end"  # Retourner quand m√™me la r√©ponse (avec warning)


def build_cov_rag_graph(enable_cove: bool = True) -> StateGraph:
    """
    Construit le graphe COV-RAG.
    
    Args:
        enable_cove: Active/d√©sactive la v√©rification CoVE
        
    Returns:
        StateGraph compil√©
    """
    workflow = StateGraph(COVRAGGraphState)
    
    # Ajouter les n≈ìuds
    workflow.add_node("retrieve", retrieve_with_rerank)
    workflow.add_node("generate", generate_initial)
    workflow.add_node("fallback", fallback_response)
    workflow.add_node("human_review", human_review)
    workflow.add_node("evaluate", evaluate_final)
    
    if enable_cove:
        workflow.add_node("extract_claims", extract_claims)
        workflow.add_node("verify_claims", verify_claims)
        workflow.add_node("correct", correct_if_needed)
    
    # Point d'entr√©e
    workflow.set_entry_point("retrieve")
    
    # Ar√™tes conditionnelles apr√®s r√©cup√©ration
    workflow.add_conditional_edges(
        "retrieve",
        decide_after_retrieval,
        {
            "generate": "generate",
            "fallback": "fallback"
        }
    )
    
    # Pipeline CoVE ou direct vers √©valuation
    if enable_cove:
        workflow.add_edge("generate", "extract_claims")
        workflow.add_edge("extract_claims", "verify_claims")
        workflow.add_edge("verify_claims", "correct")
        workflow.add_edge("correct", "evaluate")
    else:
        workflow.add_edge("generate", "evaluate")
    
    # D√©cision apr√®s √©valuation
    workflow.add_conditional_edges(
        "evaluate",
        decide_after_evaluation,
        {
            "end": END,
            "human_review": "human_review"
        }
    )
    
    # Terminaisons
    workflow.add_edge("fallback", END)
    workflow.add_edge("human_review", END)
    
    return workflow.compile()


# Graphes pr√©-compil√©s
cov_rag_app = build_cov_rag_graph(enable_cove=True)
rag_app = build_cov_rag_graph(enable_cove=False)


# ============================================================================
# API SIMPLIFI√âE
# ============================================================================

def run_cov_rag(
    question: str,
    collection: str = "demo_public",
    sources_filter: List[str] = None,
    enable_cove: bool = True
) -> Dict[str, Any]:
    """
    Ex√©cute le pipeline COV-RAG de mani√®re synchrone.
    
    Args:
        question: Question de l'utilisateur
        collection: Collection Qdrant √† interroger
        sources_filter: Filtres optionnels sur les sources
        enable_cove: Active la v√©rification CoVE
        
    Returns:
        Dict avec la r√©ponse et les m√©triques
    """
    initial_state = {
        "question": question,
        "collection": collection,
        "sources_filter": sources_filter or [],
        "cove_enabled": enable_cove
    }
    
    app = cov_rag_app if enable_cove else rag_app
    
    final_state = {}
    for output in app.stream(initial_state):
        for key, value in output.items():
            final_state.update(value)
    
    return {
        "answer": final_state.get("generation", ""),
        "sources": final_state.get("sources", []),
        "confidence": final_state.get("final_confidence", final_state.get("confidence_score", 0.0)),
        "quality_pass": final_state.get("quality_pass", False),
        "hallucination_detected": final_state.get("hallucination_detected", False),
        "corrections_made": final_state.get("corrections_made", 0),
        "language": final_state.get("response_lang", "fr"),
        "escalated": final_state.get("escalated", False)
    }


# ============================================================================
# TESTS
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("üß™ TEST COV-RAG GRAPH")
    print("=" * 70)
    
    tests = [
        {"question": "Ma carte bancaire est bloqu√©e, que dois-je faire ?", "lang": "FR"},
        {"question": "How can I dispute an unauthorized transaction?", "lang": "EN"}
    ]
    
    for idx, test in enumerate(tests, 1):
        print(f"\n{'='*70}")
        print(f"üìù TEST {idx}: {test['lang']} - {test['question']}")
        print("=" * 70)
        
        result = run_cov_rag(
            question=test["question"],
            collection="demo_public",
            enable_cove=True
        )
        
        print("\nüìä R√âSULTATS:")
        print(f"  Confiance: {result['confidence']:.0%}")
        print(f"  Quality pass: {result['quality_pass']}")
        print(f"  Hallucination: {result['hallucination_detected']}")
        print(f"  Corrections: {result['corrections_made']}")
        print(f"  Sources: {len(result['sources'])}")
        
        print("\nüí¨ R√âPONSE:")
        answer = result['answer']
        print(answer[:500] + "..." if len(answer) > 500 else answer)
    
    print("\n" + "=" * 70)
    print("‚úÖ Tests termin√©s")
    print("=" * 70)
