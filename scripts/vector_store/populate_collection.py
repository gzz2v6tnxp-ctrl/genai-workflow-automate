import sys
from pathlib import Path
import uuid
from typing import List

from qdrant_client import QdrantClient, models
from langchain_core.documents import Document

# Ajouter le r√©pertoire racine du projet au path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from scripts import config
from scripts.embed import generate_embeddings
from scripts.ingest.ingest_synth import load_synth_docs
from scripts.ingest.ingest_cfpb import load_cfpb_docs
from scripts.ingest.ingest_enron_mail import load_enron_docs
from scripts.chunking import chunk_documents

# --- Noms des Collections ---
PUBLIC_COLLECTION_NAME = "demo_public"
MAIN_KB_COLLECTION_NAME = "knowledge_base_main"


def load_all_documents(limit_per_source: int = None) -> List[Document]:
    """Charge et d√©coupe intelligemment les documents."""
    all_docs = []
    print("--- Chargement des documents depuis toutes les sources ---")

    # Synth : g√©n√©ralement courts, mais on applique quand m√™me le chunking au cas o√π
    print("Source: synth...")
    synth_docs = load_synth_docs()
    synth_chunked = chunk_documents(synth_docs, chunk_size=600)
    all_docs.extend(synth_chunked)

    # CFPB : plaintes souvent longues, chunking n√©cessaire
    print("Source: cfpb...")
    cfpb_docs = load_cfpb_docs(limit=limit_per_source)
    cfpb_chunked = chunk_documents(cfpb_docs, chunk_size=600)
    all_docs.extend(cfpb_chunked)

    # Enron : emails de longueurs variables, chunking pour les longs threads
    print("Source: enron...")
    enron_docs = load_enron_docs(limit=limit_per_source)
    enron_chunked = chunk_documents(enron_docs, chunk_size=600)
    all_docs.extend(enron_chunked)

    print(f"\n‚úÖ Total de {len(all_docs)} chunks pr√™ts pour embedding.")
    return all_docs


def upsert_data_to_collection(client: QdrantClient, collection_name: str, documents: List[Document]):
    """
    G√©n√®re les embeddings et ins√®re les documents dans une collection Qdrant sp√©cifi√©e.
    """
    if not documents:
        print(f"Aucun document √† ins√©rer dans '{collection_name}'.")
        return

    print(f"\n--- Traitement pour la collection '{collection_name}' ---")

    # 1. G√©n√©rer les embeddings
    embeddings = generate_embeddings(documents)

    # 2. Pr√©parer les points pour Qdrant
    points = []
    for i, doc in enumerate(documents):
        # Tenter de convertir l'ID existant en UUID. S'il √©choue ou n'existe pas, en g√©n√©rer un nouveau.
        try:
            doc_id = str(uuid.UUID(str(doc.metadata.get("id"))))
        except (ValueError, TypeError):
            # G√©n√®re un UUID bas√© sur le contenu si aucun ID valide n'est trouv√©
            doc_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content))

        payload = doc.metadata.copy()
        payload["page_content"] = doc.page_content
        
        points.append(
            models.PointStruct(
                id=doc_id, # Utiliser directement doc_id qui est maintenant un UUID valide
                vector=embeddings[i].tolist(),
                payload=payload,
            )
        )

    # 3. Ins√©rer les points dans Qdrant par lots
    batch_size = 100  # Taille du lot (ajustable selon la taille de vos documents)
    total_batches = (len(points) + batch_size - 1) // batch_size
    
    print(f"Insertion de {len(points)} points dans '{collection_name}' par lots de {batch_size}...")
    
    for i in range(0, len(points), batch_size):
        batch = points[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        
        try:
            client.upsert(
                collection_name=collection_name,
                points=batch,
                wait=True,
            )
            print(f"  ‚úì Lot {batch_num}/{total_batches} ins√©r√© ({len(batch)} points)")
        except Exception as e:
            print(f"  ‚úó Erreur lors de l'insertion du lot {batch_num} : {e}")
            raise
    
    print(f"‚úÖ Insertion dans '{collection_name}' termin√©e.")


def run_populate_collections(limit: int = 0):
    """
    Point d'entr√©e principal pour peupler les bases de donn√©es vectorielles.
    """
    all_documents = load_all_documents(limit_per_source=limit)
    if not all_documents:
        print("Aucun document √† traiter. Arr√™t du script.")
        return

    # synth_documents = [doc for doc in all_documents if doc.metadata.get("source") == "synth"]

    try:
        client = QdrantClient(host=config.QDRANT_HOST, port=config.QDRANT_PORT, timeout=30000)
        print(f"\nüîó Connect√© √† Qdrant sur {config.QDRANT_HOST}:{config.QDRANT_PORT}")

        # upsert_data_to_collection(client, PUBLIC_COLLECTION_NAME, synth_documents)
        upsert_data_to_collection(client, MAIN_KB_COLLECTION_NAME, all_documents)

        print("\n--- V√©rification finale du nombre de points ---")
        public_count = client.count(collection_name=PUBLIC_COLLECTION_NAME, exact=True)
        main_kb_count = client.count(collection_name=MAIN_KB_COLLECTION_NAME, exact=True)
        print(f"üìä Collection '{PUBLIC_COLLECTION_NAME}' : {public_count.count} points")
        print(f"üìä Collection '{MAIN_KB_COLLECTION_NAME}' : {main_kb_count.count} points")

    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'op√©ration avec Qdrant : {e}")

if __name__ == "__main__":
    TEST_LIMIT = 200
    run_populate_collections(limit=TEST_LIMIT)


# def main(limit: int = None):
#     """
#     Point d'entr√©e principal pour peupler les bases de donn√©es vectorielles.
#     """
#     # 1. Charger tous les documents
#     all_documents = load_all_documents(limit_per_source=limit)
#     if not all_documents:
#         print("Aucun document √† traiter. Arr√™t du script.")
#         return

#     # 2. S√©parer les documents par destination
#     synth_documents = [doc for doc in all_documents if doc.metadata.get("source") == "synth"]

#     # 3. Initialiser le client Qdrant
#     try:
#         client = QdrantClient(host=config.QDRANT_HOST, port=config.QDRANT_PORT)
#         print(f"\nConnect√© √† Qdrant sur {config.QDRANT_HOST}:{config.QDRANT_PORT}")

#         # 4. Peupler la collection publique
#         upsert_data_to_collection(client, PUBLIC_COLLECTION_NAME, synth_documents)

#         # 5. Peupler la collection principale
#         upsert_data_to_collection(client, MAIN_KB_COLLECTION_NAME, all_documents)

#         # 6. V√©rification finale
#         print("\n--- V√©rification finale du nombre de points ---")
#         public_count = client.count(collection_name=PUBLIC_COLLECTION_NAME, exact=True)
#         main_kb_count = client.count(collection_name=MAIN_KB_COLLECTION_NAME, exact=True)
#         print(f"Collection '{PUBLIC_COLLECTION_NAME}' contient : {public_count.count} points.")
#         print(f"Collection '{MAIN_KB_COLLECTION_NAME}' contient : {main_kb_count.count} points.")

#     except Exception as e:
#         print(f"\nUne erreur est survenue lors de l'op√©ration avec Qdrant : {e}")


# if __name__ == "__main__":
#     # Pour un test rapide, on peut limiter le nombre de documents par source (sauf synth)
#     # Mettre √† None pour tout traiter (attention, peut √™tre long)
#     TEST_LIMIT = 500
#     main(limit=TEST_LIMIT)