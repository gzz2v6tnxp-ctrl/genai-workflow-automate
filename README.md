```
# ğŸ¤– GenAI Workflow Automation

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [Technologies](#technologies)
- [Installation](#installation)
- [Configuration](#configuration)
- [Pipeline de DonnÃ©es](#pipeline-de-donnÃ©es)
- [API Endpoints](#api-endpoints)
- [Migration Cloud](#migration-cloud)
- [DÃ©veloppement](#dÃ©veloppement)
- [Data & Licences](#data--licences)
- [RÃ©fÃ©rences](#rÃ©fÃ©rences)

---

## ğŸ¯ Vue d'ensemble

**GenAI Workflow Automation** est une solution MVP de traitement automatisÃ© de tickets clients du secteur financier utilisant une architecture **RAG (Retrieval-Augmented Generation)** avec LLM. Le systÃ¨me permet de :

- âœ… IngÃ©rer et vectoriser des documents provenant de multiples sources
- âœ… Effectuer une recherche sÃ©mantique performante sur une base de connaissances distribuÃ©e
- âœ… GÃ©nÃ©rer des rÃ©ponses contextualisÃ©es via LLM (OpenAI GPT)
- âœ… Orchestrer des workflows complexes avec LangGraph
- âœ… DÃ©ployer en production avec Qdrant Cloud

### ğŸª Cas d'usage

**Secteur** : Services Financiers (Banque, Assurance, FinTech)

**ProblÃ©matique** : Automatiser le traitement de tickets clients (plaintes, demandes d'information) avec un systÃ¨me intelligent capable de comprendre le contexte et fournir des rÃ©ponses pertinentes basÃ©es sur l'historique et la documentation interne.

**Solution** : Pipeline RAG multi-sources combinant recherche vectorielle et gÃ©nÃ©ration LLM.

---

## ğŸ—ï¸ Architecture

### Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Synthetic Docs (100 docs, FR/EN)                             â”‚
â”‚  â€¢ CFPB Complaints (10K records, EN)                            â”‚
â”‚  â€¢ Enron Emails (Corporate communication, EN)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Document Loading (LangChain Document abstraction)           â”‚
â”‚  2. Text Chunking (RecursiveCharacterTextSplitter)              â”‚
â”‚     â€¢ Chunk size: 600 chars (~384 tokens)                       â”‚
â”‚     â€¢ Overlap: 100 chars                                        â”‚
â”‚  3. Embedding Generation (sentence-transformers)                â”‚
â”‚     â€¢ Model: all-mpnet-base-v2                                  â”‚
â”‚     â€¢ Dimension: 768                                            â”‚
â”‚  4. Batch Insertion (100 points/batch)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VECTOR DATABASE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Qdrant (Docker local + Cloud)                                  â”‚
â”‚  â€¢ Collection: demo_public (synthetic only)                     â”‚
â”‚  â€¢ Collection: knowledge_base_main (all sources)                â”‚
â”‚  â€¢ Distance metric: COSINE                                      â”‚
â”‚  â€¢ Snapshots: Automated backup/restore                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RETRIEVAL SYSTEM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Semantic search with filters                                 â”‚
â”‚  â€¢ Top-k results with score threshold                           â”‚
â”‚  â€¢ Metadata filtering (source, date, etc.)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LANGGRAPH WORKFLOW                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  retrieve â†’ grade_documents â†’ generate / fallback               â”‚
â”‚  â€¢ State management                                             â”‚
â”‚  â€¢ Conditional routing                                          â”‚
â”‚  â€¢ Error handling                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM GENERATION                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OpenAI GPT (via LangChain)                                     â”‚
â”‚  â€¢ Contextualized response generation                           â”‚
â”‚  â€¢ Source citation                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API LAYER (FastAPI)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ /search : Semantic search                                    â”‚
â”‚  â€¢ /build-collections : Collection management                   â”‚
â”‚  â€¢ /populate-collections : Data ingestion                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LangGraph Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   retrieve   â”‚  â† Recherche sÃ©mantique dans Qdrant
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ grade_documents  â”‚  â† Ã‰valuation de la pertinence
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚
       â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ generate â”‚   â”‚ fallback â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  END   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technologies

### Stack Technique

| Composant | Technologie | Version | Usage |
|-----------|-------------|---------|-------|
| **Orchestration** | LangGraph | Latest | Workflow management |
| **LLM Framework** | LangChain | >=0.0.278 | RAG pipeline |
| **LLM Provider** | OpenAI | Latest | Text generation |
| **Vector DB** | Qdrant | >=1.14.2 | Semantic search |
| **Embeddings** | Sentence-Transformers | >=2.2.2 | Text vectorization |
| **API Framework** | FastAPI | Latest | REST API |
| **Server** | Uvicorn | Latest | ASGI server |
| **Data Processing** | Pandas, NumPy | Latest | Data manipulation |
| **Environment** | Python-dotenv | Latest | Config management |
| **Testing** | Pytest | Latest | Unit tests |
| **UI Demo** | Gradio | >=3.14.0 | Interactive demo |

### ModÃ¨les ML

- **Embedding Model** : `all-mpnet-base-v2`
  - Dimension : 768
  - Max tokens : 384
  - Languages : Multilingual (50+ languages)
  - Performance : SOTA sur SBERT benchmarks

- **LLM** : OpenAI GPT-3.5/4
  - Task : Response generation
  - Context window : 16K+ tokens

---

## ğŸ“¦ Installation

### PrÃ©requis

```bash
Python >= 3.9
Docker >= 20.10 (pour Qdrant local)
Git
```

### Installation des dÃ©pendances

```bash
# Cloner le repository
git clone https://github.com/gzz2v6tnxp-ctrl/genai-workflow-automate.git
cd genai-workflow-automate

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
.\venv\Scripts\activate   # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### Lancer Qdrant (Docker)

```bash
docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant:latest
```

---

## âš™ï¸ Configuration

### Variables d'environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet :

```env
# Qdrant Local
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Qdrant Cloud (pour production)
QDRANT_CLOUD_URL=https://your-cluster.aws.cloud.qdrant.io
QDRANT_API_KEY=your-api-key-here

# Embedding Configuration
VECTOR_DIMENSION=768
DEFAULT_EMBEDDING_MODEL=all-mpnet-base-v2

# OpenAI API
OPENAI_API_KEY=sk-...

# Collection Names
COLLECTION_NAME=genai_workflow_docs_test
```

### Structure de Configuration

```python
# scripts/config.py
- QDRANT_HOST / PORT : Qdrant local instance
- QDRANT_CLOUD_URL / API_KEY : Production cluster
- VECTOR_DIMENSION : Embedding dimension (768)
- DEFAULT_EMBEDDING_MODEL : Model name
- OPENAI_API_KEY : LLM API key
```

---

## ğŸ“Š Pipeline de DonnÃ©es

### 1. Ingestion des Sources

```bash
# Charger les donnÃ©es synthÃ©tiques
python scripts/ingest/ingest_synth.py

# Charger les plaintes CFPB
python scripts/ingest/ingest_cfpb.py

# Charger les emails Enron
python scripts/ingest/ingest_enron_mail.py
```

### 2. CrÃ©ation des Collections

```bash
# CrÃ©er les collections Qdrant
python scripts/vector_store/build_collection.py
```

**Collections crÃ©Ã©es** :
- `demo_public` : 100 docs synthÃ©tiques (demo publique)
- `knowledge_base_main` : ~5000 chunks (production)

### 3. GÃ©nÃ©ration des Embeddings et Population

```bash
# GÃ©nÃ©rer les embeddings et peupler Qdrant
python scripts/vector_store/populate_collection.py
```

**Process** :
1. Chargement des documents depuis toutes les sources
2. Chunking avec RecursiveCharacterTextSplitter (600 chars, overlap 100)
3. GÃ©nÃ©ration des embeddings (batch processing)
4. Insertion par lots dans Qdrant (100 points/batch)

### 4. VÃ©rification

```bash
# Statistiques des collections
python scripts/vector_store/retrieve.py --count
```

---

## ğŸš€ API Endpoints

### Lancer l'API

```bash
# Mode dÃ©veloppement
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Mode production
uvicorn main:app --workers 4 --host 0.0.0.0 --port 8000
```

### Endpoints Disponibles

#### 1. Recherche SÃ©mantique

```http
POST /api/retriever/search
Content-Type: application/json

{
  "query": "problÃ¨me de carte de crÃ©dit refusÃ©e",
  "collection_name": "knowledge_base_main",
  "top_k": 5,
  "score_threshold": 0.7,
  "filters": {
    "source": "cfpb_complaints"
  }
}
```

**Response** :
```json
{
  "results": [
    {
      "id": "uuid",
      "score": 0.89,
      "content": "Document content...",
      "metadata": {
        "source": "cfpb_complaints",
        "product": "Credit card",
        "issue": "Transaction declined"
      }
    }
  ],
  "total": 5
}
```

#### 2. Compter les Documents

```http
GET /api/retriever/count?collection_name=knowledge_base_main
```

#### 3. RÃ©cupÃ©rer un Document par ID

```http
GET /api/retriever/documents/{document_id}?collection_name=knowledge_base_main
```

#### 4. Gestion des Collections

```http
POST /api/ingestion/build-collections
POST /api/ingestion/populate-collections
```

### Documentation Interactive

AccÃ©dez Ã  la documentation Swagger :
```
http://localhost:8000/docs
```

---

## â˜ï¸ Migration Cloud

### Processus de Migration

Consultez le guide dÃ©taillÃ© : [`docs/MIGRATION_GUIDE.md`](docs/MIGRATION_GUIDE.md)

#### MÃ©thode Automatique

```bash
# Migration complÃ¨te (local â†’ cloud)
python scripts/vector_store/migrate_to_cloud.py
```

#### MÃ©thode Manuelle

```bash
# 1. CrÃ©er les snapshots
python scripts/vector_store/create_snapshot.py

# 2. Uploader vers le cloud
python scripts/vector_store/restore_snapshot.py
```

### Snapshots

Les snapshots sont stockÃ©s dans `./snapshots/` :

```
snapshots/
â”œâ”€â”€ demo_public-{timestamp}.snapshot
â””â”€â”€ knowledge_base_main-{timestamp}.snapshot
```

**FonctionnalitÃ©s** :
- âœ… Backup automatique
- âœ… Compression des donnÃ©es
- âœ… Migration entre clusters
- âœ… Restauration point-in-time

---

## ğŸ‘¨â€ğŸ’» DÃ©veloppement

### Structure du Projet

```
genai-workflow-automate/
â”œâ”€â”€ agents/                    # LangGraph workflows
â”‚   â”œâ”€â”€ graph.py              # Graph definition
â”‚   â””â”€â”€ state.py              # State management
â”œâ”€â”€ backend/                   # Backend logic (future)
â”œâ”€â”€ data/                      # Raw datasets
â”‚   â”œâ”€â”€ complaints.csv/
â”‚   â”œâ”€â”€ enron_mail_20150507/
â”‚   â””â”€â”€ synth/
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ MIGRATION_GUIDE.md
â”œâ”€â”€ frontend/                  # UI (future)
â”œâ”€â”€ infra/                     # Infrastructure
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ analyse_data.ipynb
â”œâ”€â”€ router/                    # FastAPI routers
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â””â”€â”€ retriever.py
â”œâ”€â”€ scripts/                   # Data processing scripts
â”‚   â”œâ”€â”€ chunking.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ ingest/
â”‚   â”‚   â”œâ”€â”€ ingest_cfpb.py
â”‚   â”‚   â”œâ”€â”€ ingest_enron_mail.py
â”‚   â”‚   â””â”€â”€ ingest_synth.py
â”‚   â””â”€â”€ vector_store/
â”‚       â”œâ”€â”€ build_collection.py
â”‚       â”œâ”€â”€ create_snapshot.py
â”‚       â”œâ”€â”€ migrate_to_cloud.py
â”‚       â”œâ”€â”€ populate_collection.py
â”‚       â”œâ”€â”€ restore_snapshot.py
â”‚       â””â”€â”€ retrieve.py
â”œâ”€â”€ snapshots/                 # Qdrant snapshots
â”œâ”€â”€ main.py                    # FastAPI app entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Workflow de DÃ©veloppement

```bash
# 1. CrÃ©er une branche feature
git checkout -b feature/nouvelle-fonctionnalite

# 2. DÃ©velopper et tester
pytest tests/

# 3. Formater le code (optionnel)
black .
pre-commit run --all-files

# 4. Commit et push
git add .
git commit -m "feat: description de la fonctionnalitÃ©"
git push origin feature/nouvelle-fonctionnalite

# 5. CrÃ©er une Pull Request
```

### Tests

```bash
# Lancer tous les tests
pytest

# Tests avec couverture
pytest --cov=scripts --cov-report=html

# Tests spÃ©cifiques
pytest tests/test_retrieval.py
```

### Linting et Formatage

```bash
# Formater le code
black scripts/ router/ agents/

# VÃ©rifier la qualitÃ©
flake8 scripts/ router/ agents/
```

---

## ğŸ“Š Data & Licences

### Sources de DonnÃ©es

#### 1. CFPB Consumer Complaint Database

- **Source** : [Consumer Financial Protection Bureau](https://www.consumerfinance.gov/data-research/consumer-complaints/)
- **Licence** : Domaine public (US federal data)
- **Description** : Base de donnÃ©es de plaintes clients dans le secteur financier amÃ©ricain
- **Volume** : 10,000 enregistrements (subset)
- **Champs utilisÃ©s** :
  - `Consumer complaint narrative` : Description textuelle de la plainte
  - `Product` : CatÃ©gorie de produit financier
  - `Issue` : Type de problÃ¨me
  - `Company response to consumer` : RÃ©ponse de l'entreprise
- **Modifications** : Ã‰chantillonnage alÃ©atoire, nettoyage des donnÃ©es sensibles, anonymisation
- **Citation** : 
  ```
  Consumer Financial Protection Bureau. Consumer Complaint Database. 
  Retrieved from https://www.consumerfinance.gov/data-research/consumer-complaints/
  ```

#### 2. Enron Email Dataset

- **Source** : [CMU Enron Email Dataset](https://www.cs.cmu.edu/~enron/)
- **Licence** : PubliÃ© pour la recherche (public domain equivalent)
- **Description** : Corpus d'emails professionnels de la sociÃ©tÃ© Enron
- **Volume** : Subset de plusieurs milliers d'emails
- **Champs utilisÃ©s** :
  - `Subject` : Objet de l'email
  - `Body` : Corps du message
  - `From/To` : ExpÃ©diteur/Destinataire (anonymisÃ©s)
  - `Date` : Date d'envoi
- **Modifications** : Extraction de sous-ensembles pertinents, nettoyage, anonymisation des identitÃ©s
- **Citation** :
  ```
  Klimt, B., & Yang, Y. (2004). The Enron Corpus: A New Dataset for Email Classification Research. 
  European Conference on Machine Learning (ECML).
  ```

#### 3. Synthetic Financial Documents

- **Source** : GÃ©nÃ©rÃ©s spÃ©cifiquement pour ce projet
- **Licence** : MIT (open source)
- **Description** : Documents synthÃ©tiques simulant des tickets clients et documentation financiÃ¨re
- **Volume** : 100 documents
- **Langues** : FranÃ§ais, Anglais
- **Champs** :
  - `content` : Contenu textuel du document
  - `metadata` : MÃ©tadonnÃ©es structurÃ©es (type, langue, catÃ©gorie)
- **GÃ©nÃ©ration** : Template-based avec variations alÃ©atoires
- **Format** : JSONL

### ConsidÃ©rations Ã‰thiques et LÃ©gales

#### ConfidentialitÃ©

- âœ… Toutes les donnÃ©es personnelles identifiables (PII) ont Ã©tÃ© anonymisÃ©es
- âœ… Aucune information bancaire rÃ©elle n'est incluse
- âœ… Les emails Enron utilisent des donnÃ©es dÃ©jÃ  publiques et anonymisÃ©es

#### Usage AutorisÃ©

Ce projet est destinÃ© Ã  :
- ğŸ“š Recherche et dÃ©veloppement en NLP/ML
- ğŸ“ Ã‰ducation et formation
- ğŸ”¬ DÃ©monstration de concepts techniques
- ğŸ’¼ Portfolio professionnel

#### Restrictions

âŒ **Ne pas utiliser** pour :
- Production commerciale sans vÃ©rification des licences
- Traitement de donnÃ©es clients rÃ©elles sans consentement
- Prise de dÃ©cisions financiÃ¨res automatisÃ©es sans supervision humaine

### ConformitÃ© RGPD

Pour une utilisation en production avec donnÃ©es rÃ©elles :

1. **Consentement** : Obtenir le consentement explicite des utilisateurs
2. **Minimisation** : Collecter uniquement les donnÃ©es nÃ©cessaires
3. **Anonymisation** : Appliquer des techniques d'anonymisation robustes
4. **Droit Ã  l'oubli** : ImplÃ©menter des mÃ©canismes de suppression de donnÃ©es
5. **SÃ©curitÃ©** : Chiffrement des donnÃ©es sensibles (at rest + in transit)

### Datasets ComplÃ©mentaires (Recommandations)

Pour Ã©tendre le systÃ¨me :

- **Financial QA** : [FiQA Dataset](https://sites.google.com/view/fiqa/) (CC BY-SA)
- **Banking77** : [Banking Intent Dataset](https://arxiv.org/abs/2003.04807) (CC BY 4.0)
- **FinBERT** : [Financial Domain Corpus](https://huggingface.co/ProsusAI/finbert) (Apache 2.0)

---

## ğŸ“š RÃ©fÃ©rences

### Documentation Technique

- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Sentence-Transformers Documentation](https://www.sbert.net/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

### Articles de Recherche

1. **RAG Architecture**
   - Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." NeurIPS.

2. **Sentence Embeddings**
   - Reimers, N., & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." EMNLP.

3. **Vector Databases**
   - Johnson, J., Douze, M., & JÃ©gou, H. (2019). "Billion-scale similarity search with GPUs." IEEE Transactions on Big Data.

### Tutoriels et Guides

- [RAG Tutorial by LangChain](https://python.langchain.com/docs/use_cases/question_answering/)
- [Qdrant Snapshot Migration](https://qdrant.tech/documentation/database-tutorials/create-snapshot/)
- [Building Production-Ready RAG Systems](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

## ğŸ“„ Licence

Ce projet est sous licence **MIT**.

```
MIT License

Copyright (c) 2025 [Votre Nom]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

[Texte complet de la licence MIT]
```

---

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Pour contribuer :

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

### Guidelines

- Suivre les conventions PEP 8
- Ajouter des tests pour les nouvelles fonctionnalitÃ©s
- Documenter les fonctions avec docstrings
- Mettre Ã  jour le README si nÃ©cessaire

---

## ğŸ“§ Contact

**Auteur** : [Votre Nom]  
**Email** : votre.email@example.com  
**LinkedIn** : [Votre profil LinkedIn]  
**GitHub** : [@gzz2v6tnxp-ctrl](https://github.com/gzz2v6tnxp-ctrl)

---

## ğŸ™ Remerciements

- [LangChain](https://github.com/langchain-ai/langchain) pour le framework RAG
- [Qdrant](https://github.com/qdrant/qdrant) pour la base vectorielle performante
- [Sentence-Transformers](https://github.com/UKPLab/sentence-transformers) pour les modÃ¨les d'embedding
- [OpenAI](https://openai.com/) pour les capacitÃ©s LLM
- [CFPB](https://www.consumerfinance.gov/) et [CMU](https://www.cs.cmu.edu/) pour les datasets publics

---

**â­ Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  lui donner une Ã©toile sur GitHub !**